---
title: "Practical Machine Learning"
author: "Hetvee"
date: "9/5/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Executive Summary  
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.  
These 5 different ways have been classified into different classes namely:  
A: exactly according to the specification  
B: throwing the elbows to the front  
C: lifting the dumbbell only halfway  
D: lowering the dumbbell only halfway  
E: throwing the hips to the front  
Our aim is to predict the manner in which the subjects perform in the exercise. We use pml-training and pml-testing files to acquire the training and test data from http://groupware.les.inf.puc-rio.br/har.  

We build multiple models on the same and implement the most accurate one on our data.  

# Loading Libraries  
```{r, echo=TRUE}
#loading all the required packages for this project
library(knitr)
library(ggplot2)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
```
# Loading the data  
Let us now load the data from the csv files into R. We then further partition the training data into training and test data based on the classe variable.  
  
```{r, echo=TRUE}
pml.training <- read.csv("~/Documents/Hetvee/Coursera/Practical Machine Learning/pml-training.csv")
pml.testing <- read.csv("~/Documents/Hetvee/Coursera/Practical Machine Learning/pml-testing.csv")
#reading the two .csv files and sotring them into dataframes
inTrain  <- createDataPartition(pml.training$classe, p=0.7, list=FALSE) #partitioning the training set further into training and testing
training <- pml.training[inTrain, ]
testing  <- pml.training[-inTrain, ]
dim(training)
dim(testing)
```
  
  
# Data Cleaning  
Let us now clean the data before we begin processing it. Firstly, we remove all the NA values from the dataset.  
```{r, echo=TRUE}
navalues<-sapply(training, function(x) mean(is.na(x))) > 0.95
training<-training[, navalues==FALSE] #removing all the NA values from training set
testing<-testing[, navalues==FALSE]#removing all the NA values from the testing set
dim(training)
dim(testing)
```
  
We now remove all the values with Nearly Zero Variances.  
```{r, echo=TRUE}
nzv <- nearZeroVar(training)
training <- training[, -nzv]#removing all the NZV values from training set
testing  <- testing[, -nzv]#removing all the NZV values from test set
dim(training)
dim(testing)
```
  
We now remove all the columns with the preliminary information that is not required in our model.  
```{r, echo=TRUE}
training<-training[, -(1:5)]
testing<-testing[, -(1:5)]
dim(training)
dim(testing)
```
  
# Exploratory Data Analysis  
  
```{r, echo=TRUE}
prop.table(table(training$classe)) #finding the proportion of each class and plotting a table
```
  
  
# Building Models  
  
There are various prediction models in Machine Learning that use the concepts of repression and decision algorithms. We aim to use decision trees, randome forests, linear discriminant analysis and boosting to predict the 20 different cases given in the dataset and apply the most accurate technique on the entire data for further predictions.    
### Decision Tree  
  
Decision Trees don't usually give a high accuracy. Let us see what value we get for the model fit in this case.  
```{r, echo=TRUE}
modFit_dt <- train(classe ~ ., data = training, method="rpart")
fancyRpartPlot(modFit_dt$finalModel) #plotting the decision tree
prediction <- predict(modFit_dt, testing)#predicting the model
confusionMatrix(prediction, testing$classe)
```
  
We can note that the accuracy of this model is **0.5244**.  
  
### Random Forest  
```{r, echo=TRUE}
set.seed(233)
modFit_rf <- randomForest(classe ~ ., data = training, ntree = 100)
prediction <- predict(modFit_rf, testing)
confusionMatrix(prediction, testing$classe)
qplot(training$roll_belt, magnet_dumbbell_y, colour=classe, data=training)  
```
  
The accuray of this model is **0.9976** which means that this method is highly accurate.  
  
### Linear Discriminant Ananlysis  
  
```{r, echo=TRUE}
modFit_lda<-train(classe~., data=training, method="lda")
prediction <- predict(modFit_lda, testing)
confusionMatrix(prediction, testing$classe)
```
  
The accuracy of this model comes upto **0.715** which is greater than that of decision trees but less accurate than the output of Random Forest.  
  
# Selecting and Applying the appropriate model  
  
Since the Random Forest Modelling method gives us the most accurate results for the training set we will use it to make predictions in our final model. Given below are the 20 different test cases that are predicted from the model.  
  
```{r, echo=TRUE}
final_prediction<-predict(modFit_rf, newdata=pml.testing)
final_prediction
```
  
